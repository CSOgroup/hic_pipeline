# Hi-C processing pipeline

## About the pipeline
This pipeline is heavily inspired by the [juicer pipeline](https://github.com/aidenlab/juicer). For any information about the internal workings of the pipeline, please check their repository. You can see this pipeline as a wrapper around that one.

The main usage of this pipeline is to run the analysis of multiple Hi-C samples and organize their results in a coherent way.

## Using a pre-made setup
If the pipeline was already installed in your system, simply run the following:
```
install_hic_pipeline.sh
```
If requested, logout from the system, login again, and run again `install_hic_pipeline.sh` to complete the setup. If not, move to the next step.

## Testing the pipeline
Test that the pipeline works by running:
```
run_hic_pipeline.sh test_input.csv
```
and 
```
run_merge_hic_pipeline.sh test_mega.csv
```

where `test_input.csv` and `test_mega.csv` are sample files which can be downloaded from this Github repository. Remember to modify accordingly the `genome_sequence` and `chromsizes` fields to point to your juicer installation.

## 1) Processing single Hi-C samples
This is the main step of the pipeline.
```
./run_hic_pipeline.sh <input-samples.csv>
```

### Input format
The `run_hic_pipeline.sh` script accepts a .csv file (with header) as input having one line for each sample to be processed. The required columns are:
- *sample_path*: path to the sample results
- *raw_path*: path to the sample fastq files. Fastq files are assumed to be paired-ended and located in the same folder. Read1 and Read2 are denoted by `_R1_` and `_R2_` inside the file names.
- *restriction_enzyme*: which restriction enzyme to use (`MboI`, `HindIII`, `Arima`, etc...)
- *genome_assembly*: genome assembly (`hg19`, `mm10`, etc...)
- *genome_sequence*: path to the fasta file for the reference genome
- *chromsizes*: path to the chromosome sizes file for the reference genome

You can check the [test_input.csv](./test_input.csv) file for reference.

### Steps
The pipeline will run the following analyses:
1. Fastq quality control with `fastqc`
2. Read alignment and `.hic` file generation using `juicer`
3. Conversion of `.hic` files to `.mcool` files for compatibility with [cooler](https://github.com/open2c/cooler) format

## 2) Aggregating replicates into _mega-maps_
If you have multiple replicates of the same experiment, most likely you will want to merge them in a single file, to improve data depth and following analyses. To do that you can run:
```
./run_merge_hic_pipeline.sh <mega-samples.csv>
```

### Input format
The `run_merge_hic_pipeline.sh` script accepts a .csv file (with header) as input having one line for each aggregated hi-c map. The required columns are:
- *sample_path*: path to the aggregated map results
- *restriction_enzyme*: which restriction enzyme to use (`MboI`, `HindIII`, `Arima`, etc...). Notice that this implies that you cannot merge Hi-C samples which have been generated by different restriction enzymes
- *genome_assembly*: genome assembly (`hg19`, `mm10`, etc...). For obvious reasons, you cannot merge Hi-C samples which have been generated by different genome assemblies
- *chromsizes*: path to the chromosome sizes file for the reference genome. Same reasoning as previous column.
- *replicate_paths*: paths to the replicate sample results (generated by the previous step), separated by colon (:). 

You can check the [test_mega.csv](./test_mega.csv) file for reference.



## Installing the pipeline from scratch
Clone the repository and enter in the folder:

```
git clone https://github.com/CSOgroup/hic_pipeline.git
cd hic_pipeline
```

Install the dependencies using conda/mamba, creating a new environment (`hic_pipeline`):
```
./install_hic_pipeline.sh
```
If requested, logout from the system, login again, and run again `install_hic_pipeline.sh` to complete the setup. If not, move to the next step.